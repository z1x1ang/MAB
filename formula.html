<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>多臂老虎机算法详解</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
        body {
            font-family: Arial, sans-serif;
        }
        .formula-explanation {
            margin: 20px 0;
        }
    </style>
</head>
<body>
    <h1>多臂老虎机算法详解</h1>
    <div class="formula-explanation">
        <p>
            多臂老虎机问题中的核心是估计每个动作的价值。我们使用 \(Q_t(a)\) 来表示在时间 \(t\) 时动作 \(a\) 的估计价值。
        </p>
        <p>
            每当动作 \(a\) 被选中时，我们会收到一个数值奖励 \(R\)，我们的任务是更新 \(Q\) 以更好地反映每个动作的预期奖励。
        </p>
        <p>
            我们使用以下增量式公式来更新估计值：
            \[
            Q_{n+1} = Q_n + \frac{1}{n}(R_n - Q_n)
            \]
        </p>
        <ul>
            <li>\(Q_{n+1}\) 是动作 \(a\) 在第 \(n+1\) 次选择后的估计价值。</li>
            <li>\(Q_n\) 是动作 \(a\) 在第 \(n\) 次选择前的估计价值。</li>
            <li>\(R_n\) 是动作 \(a\) 在第 \(n\) 次选择时收到的实际奖励。</li>
            <li>\(n\) 是到目前为止动作 \(a\) 被选中的次数。</li>
        </ul>
        <p>
            这个公式的工作方式是，每次动作被选择时，它都会根据新的奖励信息调整估计值。特别是，它将新的奖励 \(R_n\) 与当前的估计值 \(Q_n\) 进行比较，并按比例 \(1/n\) 调整估计值。
        </p>
        <p>
            在第一次迭代 \(n=1\) 时，\(Q_1\) 通常被初始化为 0 或另一个中性值，因为在收到首次奖励之前，我们没有关于预期奖励的任何信息。
        </p>
        <p>
            此公式确保了估计值会随着时间的推移逐渐接近真实的奖励平均值，因为每个新的奖励样本都会微调当前的估计。
        </p>
        <p>
            如果我们采用简单平均的方法来计算 \(Q_{n+1}\)，我们可以这样做：
            \[
            Q_{n+1} = \frac{1}{n+1}(R_1 + R_2 + ... + R_n + R)
            \]
            这里，\(R_1\) 到 \(R_n\) 是之前的奖励，而 \(R\) 是新的奖励。
        </p>
        <p>
            我们可以重写上面的表达式，将 \(Q_n\)（之前奖励的平均值）包含进去：
            \[
            Q_{n+1} = \frac{1}{n+1}(nQ_n + R)
            \]
            这可以进一步改写为：
            \[
            Q_{n+1} = Q_n + \frac{1}{n+1}(R - Q_n)
            \]
        </p>
        <p>
            这个表达式说明了每次接收新奖励时，我们是如何根据新奖励和之前的平均奖励来更新 \(Q_{n+1}\) 的。这个过程持续进行，使得 \(Q\) 的值越来越接近真实的期望奖励值。
        </p>
    </div>
</body>
</html>
